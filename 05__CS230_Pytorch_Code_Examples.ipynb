{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Pytorch Code Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training, model, loss function, optimzer에 대한 내용입니다. [CS230 link](https://cs230.stanford.edu/blog/pytorch/)\n",
    "\n",
    "이 노트는 CS230 Project Code Example로 발표한 [메인](https://cs230.stanford.edu/blog/tips/) 포스트에 대한 팔로우 내용이며 스탠포드의 [github 레파지토리](https://github.com/cs230-stanford/cs230-code-examples) 코드에 대한 디테일한 설명입니다.\n",
    "```\n",
    "pytorch/\n",
    "    vision/\n",
    "    nlp/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 튜토리얼은 아래 코드 예제 시리즈 중 하나입니다\n",
    "* [설치, 프로젝트 시작](https://cs230.stanford.edu/blog/tips/)\n",
    "* (이 노트에 해당) Pytorch code의 전반적인 구조\n",
    "* [hand sign 이미지에 대한 레이블 예측](https://cs230.stanford.edu/blog/handsigns/)\n",
    "* [개체명인식(NER)](https://cs230.stanford.edu/blog/namedentity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이 튜토리얼의 목표\n",
    "* Pytorch에 대한 이해\n",
    "* 어떻게 정확하게 Pytorch 딥러닝 프로젝트 구조화하는지에 대한 예제\n",
    "* 필요에 맞는 수정을 하기 위하여 잘 짜여진 코드 이해하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참고 자료\n",
    "* [Pytorch 사이트](https://pytorch.org/)\n",
    "* [Pytorch 공식 튜토리얼](https://pytorch.org/tutorials/)은 다양한 유스케이스들을 다룹니다.(ex. attention based seq2seq model, Deep Q-Net, Neural transfer 등)\n",
    "* [Pytorch 60분 리뷰](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "* Justin Johnson의 [git repository](https://github.com/jcjohnson/pytorch-examples), 자체 예제를 통해 Pytorch 컨셉들에 대한 기초적 부분을 소개합니다.\n",
    "* [그 밖에](https://github.com/ritchieng/the-incredible-pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코드 레이아웃\n",
    "가각의 Pytorch 예제에 대한 코드는 아래 공통적인 구조를 공유합니다\n",
    "\n",
    "```\n",
    "data/\n",
    "experiments/\n",
    "model/\n",
    "    net.py\n",
    "    data_loader.py\n",
    "train.py\n",
    "evaluate.py\n",
    "search_hyperparams.py\n",
    "synthesize_results.py\n",
    "evalutate.py\n",
    "utils.py\n",
    "```\n",
    "* `model/net.py` : 뉴럴 네트워크 구조와 손실 함수 그리고 평가 지표을 지정합니다\n",
    "* `model/data_loader.py`: 데이터가 네트워크에 어떻게 피드되는지\n",
    "* `train.py`: 메인 학습 loop 내용을 포함합니다\n",
    "* `evaluate.py`: 모델 평가를 위한 메인 loop 내용을 포함합니다\n",
    "* `utils.py`: 하이퍼파라미터, 로깅, 모델 저장에 대한 유용한 함수들\n",
    "\n",
    "높은 수준의 개요를 위해 `train.py`를 읽기를 추천합니다\n",
    "\n",
    "업무 태스크와 데이터셋에 따라 높은 수준의 아이디어가 생기면, 아마 수정을 하고 싶을 수도 있습니다\n",
    "* `model/net.py`: 모델 변경, 즉 입력을 예측과 손실로 변환하는 방법 등\n",
    "* `model/data_loader.py`: 모델에 데이터를 공급하는 방식을 변경합니다.\n",
    "* `train.py`와 `evaluate.py`: 필요하다면, 각각의 문제에대한 구체적인 변경\n",
    "\n",
    "데이터셋에 대한 작업을 하며 무언가를 얻는다면, 나에게 맞는 필요에 맞춰 코드의 어떤 부분이라도 부담없이 수정하세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텐서와 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시작전에, pytorch 기초에 대한 이해를 얻기 위하여 [Pytorch 60분 리뷰](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)를 먼저 보시는걸 강력히 추천합니다. 아래는 맛보기 샘플입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch 텐서는 Numpy array와 동작이 비슷합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.Tensor([[1,2],[3,4]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  4.],\n",
      "        [ 9., 16.]])\n"
     ]
    }
   ],
   "source": [
    "print(a**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch 변수는 텐서를 감싸고 텐서에 대하여 실행할 동작을 설정할 수 있습니다. 아래는 자동 미분을 실행하도록 한 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "a = Variable(torch.Tensor([[1,2],[3,4]]), requires_grad=True)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = torch.sum(a**2)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 내용들은 앞으로의 내용에 대한 서론입니다. PyTorch는 미니멀하고 직관적 인 구문으로 우아함과 표현력을 제공합니다. 계속 진행하기 전에 \"참고 자료\" 섹션 의 몇 가지 예를 더 보기를 추천합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심 학습 단계\n",
    "학습 알고리즘의 핵심이 어떻게 보이는지 살펴 봅니다. 아래의 다섯 줄은 모델에 batch 입력을 전달하고 손실을 계산하며 역전파를 수행하고 파라미터를 업데이트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3f83c38a239e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m# compute model output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_batch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# clear previous gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# compute gradients of all variables wrt loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "output_batch = model(train_batch)           # 모델 출력 셰산\n",
    "loss = loss_fn(output_batch, labels_batch)  # 손실 계산\n",
    "\n",
    "optimizer.zero_grad()  # 이전 그래디언트들 초기화\n",
    "loss.backward()        # 모든 변수 loss에 대한 그래디언트 계산\n",
    "\n",
    "optimizer.step()       # 계산된 그래디언트를 사용하여 업데이트 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 변수 `train_batch`, `labels_batch`, `output_batch` 및 `loss` 는 PyTorch 변수이고 미분이 자동적으로 계산되도록 합니다.\n",
    "\n",
    "우리가 작성한 다른 모든 코드는 모델의 정확한 사양, 데이터와 레이블의 batch를 가져오는 방법, 손실 계산 및 옵티마이저의 상세내용을 중심으로 작성되었습니다. 이 포스트에서 PyTorch에서 간단한 모델을 작성하고 손실을 계산하고 최적화 프로그램을 정의하는 방법을 설명합니다. 다음 포스트는 각각 이미지 데이터와 텍스트 데이터에 대한 데이터 가져오는 부분을 다룹니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델은 Pytorch에서 `torch.nn.Module 클래스`를 상속받음으로써 정의될 수 있습니다. 이 모델은 2단계로 정의됩니다. 우리는 먼저 모델의 파라미터들을 정의하고 파라미터들이 어떻게 인풋에게 적용되는지 정의합니다. 학습하는 파라미터가 포함되어 있지 않은 작업들을 위해(예를들어 ReLU같은 activation fuction, maxpool같은 동작들), `torch.nn.functional` 모듈을 일반적으로 사용합니다. 여기 single hidden layer 네트워크를 통한 예제가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "\n",
    "        D_in: input dimension\n",
    "        H: dimension of hidden layer\n",
    "        D_out: output dimension\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in, H) \n",
    "        self.linear2 = nn.Linear(H, D_out)\n",
    "\n",
    "def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must \n",
    "        return a Variable of output data. We can use Modules defined in the \n",
    "        constructor as well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        h_relu = F.relu(self.linear1(x))\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 `__init__` 함수는 모델의 두 리니어 레이어를 초기화합니다. PyTorch는 우리가 지정한 파라미터에 대한 올바른 초기화를 처리합니다. 이 `forward` 함수 에서는 먼저 첫 번째 선형 레이어를 적용하고, ReLU 활성화를 적용한 다음 두 번째 선형 레이어를 적용합니다. `x`의 첫 번째 차원이 배치 크기 라고 가정합니다. 네트워크에 대한 입력이 단순한 차원 100의 벡터이고 배치 크기가 32이면 `x`에 대한 차원은 32,100이 되겠습니다. 모델을 정의하고 forward로 계산하는 방법의 예제를 살펴 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c73cf66822bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Forward 계산: x를 모델에 입력하여 예측값 y를 계산합니다\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# dim: 32 x 10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mregistered\u001b[0m \u001b[0mhooks\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlatter\u001b[0m \u001b[0msilently\u001b[0m \u001b[0mignores\u001b[0m \u001b[0mthem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \"\"\"\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# N은 배치 사이즈\n",
    "# D_in 은 인풋 차수\n",
    "# H는 히든 레이어의 차수\n",
    "# D_out은 출력 차수\n",
    "N, D_in, H, D_out = 32, 100, 50, 10\n",
    "\n",
    "# input과 output을 가질 랜덤 텐서를 생성하고 변수로 래핑합니다\n",
    "x = Variable(torch.randn(N, D_in))  # 차수: 32 x 100\n",
    "\n",
    "# 위에서 정의한 클래스를 인스턴스하여 모델을 생성합니다\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Forward 계산: x를 모델에 입력하여 예측값 y를 계산합니다\n",
    "y_pred = model(x)   # 차수: 32 x 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "더 복잡한 모델들은 같은 레이아웃을 따르며 우리는 그들중 두개를 이후 포스트에서 다룰겁니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch는 `torch.nn` [모듈](https://pytorch.org/docs/master/nn.html#loss-functions)에서 사용할 수있는 많은 표준 손실 함수들을 제공합니다. 교차 엔트로피 손실을 계산하는 방법에 대한 간단한 예는 다음과 같습니다. 모델이 `C` 레이블 의 다중 클래스 분류 문제를 해결한다고 가정해 보겠습니다. batch 사이즈인 `N`에 대하여 `out`은 모델에 인풋 배치를 입력으로 넘기면서 얻어지는 `NxC`차수의 Pytorch 변수입니다. 또한 `N` 사이즈인 각각 요소가 해당 예제의 정답인(ex. `[0,...,C-1]`의 레이블) `target` 변수가 있습니다. 손실 함수를 정의하고 다음과 같이 손실을 계산할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b34f082caa09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(out, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch는 이와 같은것들을 확장하고 커스텀한 손실 함수를 작성하는걸 매우 쉽게 해줍니다. 우리는 아래와 같이 커스텀한 Cross Entropy Loss 함수를 작성합니다. (Numpy식 코드에 주의하세요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myCrossEntropyLoss(outputs, labels):\n",
    "    batch_size = outputs.size()[0]            # batch_size\n",
    "    outputs = F.log_softmax(outputs, dim=1)   # softmax log 계삱\n",
    "    outputs = outputs[range(batch_size), labels] # label에 해당하는 값 가져오기\n",
    "    return -torch.sum(outputs)/num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이건 꽤 심플한 커스텀한 loss 함수 작성 예제였습니다. [NLP](https://cs230.stanford.edu/blog/namedentity/) 섹션에서 우리는 흥미로운 커스텀 loss 함수를 살펴볼것입니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 옵티마이저"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.optim` 패키지는 공통 최적화 알고리즘들에 대한 쉬운 인터페이스 사용을 제공합니다. 옵티마이저를 정의하는것은 아래와 같이 매우 쉽습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pick an SGD optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)\n",
    "\n",
    "#or pick ADAM\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매 iterate 마다 업데이트가 필요한 모델의 파라미터를 전달합니다. 계층 별 또는 파라미터별 learning rate 같은보다 복잡한 방법을 지정할 수도 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`loss.backward()`를 사용하여 그래디언트들이 계산하면, `optimizer.step()`은 이 파라미터들을 정의된 최적화 알고리즘 대로 업데이트합니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 vs 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 학습 전에, `model.train()`을 호출해야하는 것은 반드시 해야되는 것입니다. 이와 마찬가지로, `model.eval()`도 모델 테스트 전에 호출해야합니다. 이를 통해 교육 및 테스트 동안 dropout과 배치 정규화가 수정됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 측정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 단계까지 당신은 `train.py`와 `evaluate.py`의 대부분의 코드를 이해할 수 있어야합니다. (데이터를 어떻게 가져올지는 제외하고, 이는 이 후 포스트에서 다룰 겁니다). 손실을 모니터링 한다는 것 외에, 정확도나 precision/recall 과 같은 다른 메트릭들을 모니터링 하는 것도 도움이 됩니다. 이를 위해, `model/net.py`에서 모델 출력 배치를 위한 커스텀한 메트릭 함수들을 정의합니다. 더 쉽게하기 위해, 메트릭 함수에 전달 전에 Pytorch 변수를 Numpy array로 변환합니다. Loss Function 섹션에서 설정한것 처럼 멀티 분류 문제를 위해 Numpy를 사용하여 아래와 같이 정확도를 계산하는 함수를 작성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    return np.sum(outputs==labels)/float(labels.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model/net.py`에 커스텀한 메트릭들을 추가할 수 있습니다. 추가하면, 단순하게 `metrics` 딕셔너리에 이들을 추가할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = { 'accuracy': accuracy,\n",
    "            ## 커스텀 메트릭들을 추가,\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('py37': conda)",
   "language": "python",
   "name": "python37664bitpy37conda52cfa4cb78c54a8192e41e1627cc856b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
